# SOCAE (Sparse Orthogonal Contrastive AutoEncoder) training config
# This config trains the SOCAE bottleneck on frozen DINOv2-Base encoder outputs

# Encoder configuration (frozen, used only for extracting latents)
encoder:
  target: stage1.RAE
  params:
    encoder_cls: 'Dinov2withNorm'
    encoder_config_path: 'facebook/dinov2-with-registers-base'
    encoder_input_size: 224
    encoder_params:
      dinov2_path: 'facebook/dinov2-with-registers-base'
      normalize: true
    decoder_config_path: 'configs/decoder/ViTXL'
    noise_tau: 0.0  # No noise during SOCAE training
    reshape_to_2d: true

# SOCAE bottleneck configuration
socae:
  target: stage1.bottleneck.socae.SparseOrthogonalContrastiveAutoEncoder
  params:
    input_dim: 768  # DINOv2-Base hidden size
    hidden_dim: 16384  # 16x expansion for sparse latent (768 * 16)
    topk: 32  # Number of top activations to keep (sparsity level)
    larger_topk: 128  # Less sparse representation for better reconstruction
    dead_topk: null  # Auto-calculate based on input_dim
    dead_threshold: 64  # Batches without activation to consider neuron dead
    ncl_with_larger_k: false  # Use z_k for NCL loss (not z_large_k)
    track_dead_neuron_pre_relu: true
    activation_history_size: 1000
    encoder_bias: true  # Aligned with reference default
    z_dead_k_in_positives_for_ncl: false

# Loss configuration (aligned with reference naming)
loss:
  # Reference uses weight=1.0 for recon_k implicitly (no config needed)
  reconstruct_loss_large_k_discount: 0.125  # 1/8 - discount for large_k reconstruction
  reconstruct_loss_dead_k_discount: 0.03125  # 1/32 - discount for dead_k reconstruction
  ncl_loss_weight: 0.1  # NCL contrastive loss weight
  ortho_loss_weight: 0.1  # Orthogonality regularization weight
  ncl_temperature: 0.2  # Temperature for contrastive loss
  ncl_sim_threshold: 0.8  # Similarity threshold for NCL
  normalize_input: false  # Normalize input embedding to unit length

# Training configuration
training:
  epochs: 50
  ema_decay: 0.9999
  global_batch_size: 2048  # Large batch for contrastive learning
  num_workers: 8
  clip_grad: 1.0
  log_interval: 100
  checkpoint_interval: 5  # Every 5 epochs
  global_seed: 42
  simulated_annealing: true  # Enable simulated annealing for topk
  simulated_annealing_delay: 1000  # Delay before annealing starts (aligned with reference)
  init_from_data: true  # Initialize bias from data
  init_samples: 100000  # Number of samples for initialization
  optimizer:
    lr: 1.0e-4  # Aligned with reference default
    betas: [0.9, 0.999]
    weight_decay: 0.0
    eps: 6.25e-10  # Aligned with reference
    fused: true  # Aligned with reference
  scheduler:
    type: cosine
    warmup_epochs: 2
    decay_end_epoch: 50
    base_lr: 1.0e-4
    final_lr: 1.0e-5
    warmup_from_zero: true
