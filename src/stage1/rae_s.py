from math import sqrt
from typing import Any, Dict, Optional, Protocol, Union

import torch
import torch.nn as nn
from transformers import AutoConfig, AutoImageProcessor

from .bottleneck.socae import (
    SparseContrastiveAutoEncoder,
    SparseOrthogonalContrastiveAutoEncoder,
)
from .decoders import GeneralDecoder
from .encoders import ARCHS


def _get_obj_from_str(string: str) -> type:
    """Import a class from a module path string."""
    import importlib

    module, cls = string.rsplit(".", 1)
    return getattr(importlib.import_module(module, package=None), cls)


def _instantiate_quantizer(
    config: Union[Dict[str, Any], nn.Module, None]
) -> Optional[nn.Module]:
    """
    Instantiate a quantizer from a config dict or return the module directly.

    Args:
        config: Either a dict with 'target' and 'params' keys (and optionally 'ckpt'),
                an nn.Module instance, or None.

    Returns:
        Instantiated quantizer module or None.
    """
    if config is None:
        return None
    if isinstance(config, nn.Module):
        return config

    # Handle both regular dict and OmegaConf DictConfig
    # Check if it has dict-like behavior with 'target' key
    if hasattr(config, "get") and "target" in config:
        quantizer_cls = _get_obj_from_str(config["target"])
        # Convert params to regular dict if it's a DictConfig
        params = config.get("params", {})
        if hasattr(params, "items"):
            params = dict(params)
        quantizer = quantizer_cls(**params)

        # Load checkpoint if provided
        ckpt_path = config.get("ckpt", None)
        if ckpt_path is not None:
            print(f"Loading pre-trained quantizer from {ckpt_path}")
            state_dict = torch.load(ckpt_path, map_location="cpu")
            # Handle different checkpoint formats (ema, model, or raw state_dict)
            if "ema" in state_dict:
                state_dict = state_dict["ema"]
            elif "model" in state_dict:
                state_dict = state_dict["model"]
            quantizer.load_state_dict(state_dict, strict=True)
            print(f"Successfully loaded pre-trained quantizer from {ckpt_path}")

        return quantizer
    raise ValueError(
        f"Invalid quantizer config: expected dict-like object with 'target' key or nn.Module, got {type(config)}"
    )


class Stage1Protocal(Protocol):
    # must have patch size attribute
    patch_size: int
    hidden_size: int

    def encode(self, x: torch.Tensor) -> torch.Tensor: ...


class QuantizerProtocol(Protocol):
    def encode(self, x: torch.Tensor) -> torch.Tensor: ...

    def decode(self, z: torch.Tensor) -> torch.Tensor: ...


Quantizer = Union[
    SparseContrastiveAutoEncoder, SparseOrthogonalContrastiveAutoEncoder, None
]


class RAE(nn.Module):
    def __init__(
        self,
        # ---- encoder configs ----
        encoder_cls: str = "Dinov2withNorm",
        encoder_config_path: str = "facebook/dinov2-base",
        encoder_input_size: int = 224,
        encoder_params: dict = {},
        # ---- decoder configs ----
        decoder_config_path: str = "vit_mae-base",
        decoder_patch_size: int = 16,
        pretrained_decoder_path: Optional[str] = None,
        # ---- quantizer configs ----
        quantizer: Quantizer = None,
        # ---- noising, reshaping and normalization-----
        noise_tau: float = 0.8,
        reshape_to_2d: bool = True,
        normalization_stat_path: Optional[str] = None,
        eps: float = 1e-5,
    ):
        super().__init__()
        encoder_cls = ARCHS[encoder_cls]
        self.encoder: Stage1Protocal = encoder_cls(**encoder_params)
        print(f"encoder_config_path: {encoder_config_path}")
        proc = AutoImageProcessor.from_pretrained(encoder_config_path)
        self.encoder_mean = torch.tensor(proc.image_mean).view(1, 3, 1, 1)
        self.encoder_std = torch.tensor(proc.image_std).view(1, 3, 1, 1)
        encoder_config = AutoConfig.from_pretrained(encoder_config_path)
        # see if the encoder has patch size attribute
        self.encoder_input_size = encoder_input_size
        self.encoder_patch_size = self.encoder.patch_size
        self.latent_dim = self.encoder.hidden_size
        assert (
            self.encoder_input_size % self.encoder_patch_size == 0
        ), f"encoder_input_size {self.encoder_input_size} must be divisible by encoder_patch_size {self.encoder_patch_size}"
        self.base_patches = (
            self.encoder_input_size // self.encoder_patch_size
        ) ** 2  # number of patches of the latent

        # decoder
        decoder_config = AutoConfig.from_pretrained(decoder_config_path)
        decoder_config.hidden_size = (
            self.latent_dim
        )  # set the hidden size of the decoder to be the same as the encoder's output
        decoder_config.patch_size = decoder_patch_size
        decoder_config.image_size = int(decoder_patch_size * sqrt(self.base_patches))
        self.decoder = GeneralDecoder(decoder_config, num_patches=self.base_patches)
        # load pretrained decoder weights
        if pretrained_decoder_path is not None:
            print(f"Loading pretrained decoder from {pretrained_decoder_path}")
            state_dict = torch.load(pretrained_decoder_path, map_location="cpu")
            keys = self.decoder.load_state_dict(state_dict, strict=False)
            if len(keys.missing_keys) > 0:
                print(
                    f"Missing keys when loading pretrained decoder: {keys.missing_keys}"
                )
        self.noise_tau = noise_tau
        self.reshape_to_2d = reshape_to_2d
        if normalization_stat_path is not None:
            stats = torch.load(normalization_stat_path, map_location="cpu")
            self.latent_mean = stats.get("mean", None)
            self.latent_var = stats.get("var", None)
            self.do_normalization = True
            self.eps = eps
            print(f"Loaded normalization stats from {normalization_stat_path}")
        else:
            self.do_normalization = False

        self.quantizer = _instantiate_quantizer(quantizer)

    def noising(self, x: torch.Tensor) -> torch.Tensor:
        noise_sigma = self.noise_tau * torch.rand(
            (x.size(0),) + (1,) * (len(x.shape) - 1), device=x.device
        )
        noise = noise_sigma * torch.randn_like(x)
        return x + noise

    @torch.no_grad()
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        # normalize input
        _, _, h, w = x.shape
        if h != self.encoder_input_size or w != self.encoder_input_size:
            x = nn.functional.interpolate(
                x,
                size=(self.encoder_input_size, self.encoder_input_size),
                mode="bicubic",
                align_corners=False,
            )
        x = (x - self.encoder_mean.to(x.device)) / self.encoder_std.to(x.device)
        z = self.encoder(x)
        if self.training and self.noise_tau > 0:
            z = self.noising(z)
        if self.reshape_to_2d:
            b, n, c = z.shape
            h = w = int(sqrt(n))
            z = z.transpose(1, 2).view(b, c, h, w)
        if self.do_normalization:
            latent_mean = (
                self.latent_mean.to(z.device) if self.latent_mean is not None else 0
            )
            latent_var = (
                self.latent_var.to(z.device) if self.latent_var is not None else 1
            )
            z = (z - latent_mean) / torch.sqrt(latent_var + self.eps)
        return z

    def decode(self, z: torch.Tensor) -> torch.Tensor:
        if self.do_normalization:
            latent_mean = (
                self.latent_mean.to(z.device) if self.latent_mean is not None else 0
            )
            latent_var = (
                self.latent_var.to(z.device) if self.latent_var is not None else 1
            )
            z = z * torch.sqrt(latent_var + self.eps) + latent_mean
        if self.reshape_to_2d:
            b, c, h, w = z.shape
            n = h * w
            z = z.view(b, c, n).transpose(1, 2)
        output = self.decoder(z, drop_cls_token=False).logits
        x_rec = self.decoder.unpatchify(output)
        x_rec = x_rec * self.encoder_std.to(x_rec.device) + self.encoder_mean.to(
            x_rec.device
        )
        return x_rec

    def quantize(self, z: torch.Tensor) -> torch.Tensor:
        if self.quantizer is None:
            return z

        if self.reshape_to_2d:
            # z is [B, C, H, W], flatten to [B, H*W, C] for quantizer
            b, c, h, w = z.shape
            z_flat = z.view(b, c, h * w).transpose(1, 2)  # [B, N, C]
        else:
            # z is [B, N, C]
            z_flat = z

        # Quantizer: encode -> sparse latent -> decode
        # The quantizer.encode applies top-k sparsification
        z_sparse = self.quantizer.encode(z_flat.reshape(-1, z_flat.shape[-1]))
        # The quantizer.decode reconstructs from sparse representation
        z_reconstructed = self.quantizer.decode(z_sparse)
        z_reconstructed = z_reconstructed.view(z_flat.shape)

        if self.reshape_to_2d:
            # Reshape back to [B, C, H, W]
            z_reconstructed = z_reconstructed.transpose(1, 2).view(b, c, h, w)

        return z_reconstructed

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        z = self.encode(x)
        z_sparse = self.quantize(z)
        x_rec = self.decode(z_sparse)
        return x_rec
